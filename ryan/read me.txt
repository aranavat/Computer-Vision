In regards to the variation of the Lunar Lander (Q-Learning/DQN and PPO)
PPO: 306 steps, 250.42 reward, Mean reward over 100 episodes: 196.13 +/- 90.97

Overall, the PPO was not only faster, but also had a greater reward. 

The Tabular Q-Learning method was simple in practice, using the epsilon-greedy strategy to hone in on higher reward actions and the Bellman equation in order to maximize q-value state.
For hyperparameters, I opted to keep them at what was provided initially at learning rate of 0.01, episodes of 50,000, starting epsilon at 1, epsilon decay at .997, and minimum epsilon at 0.1. I did not change the epsilons in particular in that I agreed that the algorithm should continue finding exploration even after it was fully decayed. However, for the most part, I did not change the hyperparameters because training the agent simply took too long, preventing "quick" adjustments to experiment for optimal results.

For DQN, the results were fairly similar to the Q-Learning method. The number of episodes was adjusted from 500 to 5000 as the reward was minimal. 

By far, not only was the PPO model conclude the fastest, but it also returned a very high reward of 250.42 in 306 steps. The speed may be attributed to switching to CPU - documentation of PPO suggests that PPO is meant to be run through it, though its precise effect is unknown.